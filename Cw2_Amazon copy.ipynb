{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1711639686314,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"STzhSkApiLIn"},"outputs":[],"source":["# !pip install SMOTE"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711639687955,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"fi10uuDuiLIo"},"outputs":[],"source":["# !pip install imbalanced-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711639688300,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"IbccvZDCiLIo"},"outputs":[],"source":["# !pip install -U scikit-learn imbalanced-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711639689100,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"LluOvvqwiLIp"},"outputs":[],"source":["# pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711639689983,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"IJn6REU8iLIp"},"outputs":[],"source":["# pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":534,"status":"ok","timestamp":1711639691169,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"ckTbwOmdiLIp"},"outputs":[],"source":["# pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711639691681,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"oevV5NR8iLIp"},"outputs":[],"source":["# pip install numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711639692114,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"Q3P0ngz3iLIp"},"outputs":[],"source":["# pip install pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711639693174,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"HecUwZCHiLIp"},"outputs":[],"source":["# pip install matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711639694660,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"a-kkKglmiLIp"},"outputs":[],"source":["# pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1711639695127,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"-p3f8fiWiLIp"},"outputs":[],"source":["# pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1711639695514,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"GUuKn0bLiLIp"},"outputs":[],"source":["# import nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711639696551,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"yCyZywvfiLIp"},"outputs":[],"source":["# nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1711639697781,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"we0uov_-nCET"},"outputs":[],"source":["# nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1711639698108,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"n6b1kwlEnQAC"},"outputs":[],"source":["# nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install gensin"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13157,"status":"ok","timestamp":1711639712882,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"0Y-IuX1SiLIq"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.metrics import classification_report\n","from sklearn.ensemble import RandomForestClassifier\n","from imblearn.over_sampling import SMOTE\n","from collections import Counter\n","from imblearn.under_sampling import RandomUnderSampler\n","# import keras.preprocessing.text Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, LSTM, Dense\n","from keras.models import Sequential\n","from imblearn.under_sampling import TomekLinks\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n","import xgboost as xgb\n","from gensim.models import KeyedVectors\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.feature_extraction.text import HashingVectorizer\n","# import spacy\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n","from sklearn.preprocessing import Normalizer\n","from gensim.models import KeyedVectors\n","from sklearn.pipeline import Pipeline\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","import re\n","from sklearn.svm import SVC\n","from sklearn.linear_model import SGDClassifier\n","from gensim.models import KeyedVectors\n","import random\n","import tensorflow as tf\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2271,"status":"ok","timestamp":1711639773147,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"gsifKTb3iLIq"},"outputs":[],"source":["train = pd.read_csv(\"./train.csv\")\n","test = pd.read_csv(\"./test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711639775095,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"wbmxlG2iiLIq","outputId":"0296d3be-7164-4c40-9bed-5e5258453e9d"},"outputs":[],"source":["train.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":532,"status":"ok","timestamp":1711639778597,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"xmh09hWimN01"},"outputs":[],"source":["def preprocess(text):\n","    pattern = r\"[^\\w\\s]\"\n","    text = [''.join([char if char.isalpha() else ' ' for char in word]) for word in text.split()]\n","    text = ' '.join(text)\n","    text =  re.sub(pattern,\" \",text)\n","     # used word_tokenize function to tokenize the text, gives list\n","\n","    tokenized_text = word_tokenize(text.lower())\n","    # get the stop words\n","    stop_words = set(stopwords.words('english'))\n","    # removed stop words\n","    tokenized_text = [word for word in tokenized_text if word not in stop_words]\n","    # applying stemming\n","    # stemmer = PorterStemmer()\n","    # tokenized_text = [stemmer.stem(word) for word in tokenized_text]\n","    # applying lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    tokenized_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n","\n","    preprocessed_text = ' '.join(tokenized_text)\n","\n","    return preprocessed_text"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1711639781685,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"qg0cXAAnpZ5R"},"outputs":[],"source":["def document_vector(word2vec,doc_tokens):\n","  tokens = [token for token in doc_tokens if token in word2vecmodel.key_to_index]\n","  if not tokens:\n","    return np.zeros(word2vec_model.vector_size)\n","  doc_vector = np.mean(word2vec_model[tokens],axis=0)\n","  return doc_vector"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reset_random_seeds(SEED=42):\n","   os.environ['PYTHONHASHSEED']=str(SEED)\n","   tf.random.set_seed(SEED)\n","   np.random.seed(SEED)\n","   random.seed(SEED)\n","reset_random_seeds(SEED=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":846,"status":"ok","timestamp":1711639810554,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"AHL99EKQiLIq","outputId":"1325778d-35b6-4d80-9dbc-3c5f705eb19b"},"outputs":[],"source":["train.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1711639813596,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"vaae7dD6iLIr","outputId":"45b1019a-04de-4864-faee-8a625c1d9d49"},"outputs":[],"source":["train.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1711639817083,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"i960SLZ9iLIr","outputId":"d157c34d-401a-4cd2-d173-bb8e7d1b343d"},"outputs":[],"source":["train.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":517,"status":"ok","timestamp":1711639820055,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"m09EUhvriLIr","outputId":"8b2a70c6-32ec-4d46-a035-b13ff161bf61"},"outputs":[],"source":["train.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":584,"status":"ok","timestamp":1711639822505,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"1Ei-xd4OiLIr"},"outputs":[],"source":["train = train.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":519,"status":"ok","timestamp":1711639824738,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"lt8ESqaMiLIr","outputId":"acf76289-554b-455d-a021-229c471c7605"},"outputs":[],"source":["train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":512,"status":"ok","timestamp":1711639826798,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"EkEJX4mMiLIr","outputId":"a5ab1cc9-4d8a-4ca4-e1ca-f8d66594d1c6"},"outputs":[],"source":["class_counts = train['overall'].value_counts()\n","\n","plt.bar(class_counts.index, class_counts.values)\n","plt.xlabel('Class')\n","plt.ylabel('Number of Samples')\n","plt.title('Histogram of Sample Counts per Class')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1711639829602,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"djVxNvSKiLIr","outputId":"dbdc9f0e-8c86-4b36-a857-210c579b0110"},"outputs":[],"source":["test.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711639832648,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"Du9nBRF5iLIr","outputId":"a80100b7-52b7-45aa-e65a-1a43663c9fe0"},"outputs":[],"source":["test.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1711639835724,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"oP3b2cX8iLIr","outputId":"46675ad9-0ae7-4aa5-d109-48005395d880"},"outputs":[],"source":["test.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":534,"status":"ok","timestamp":1711639838606,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"kIXdEjAliLIs","outputId":"59320635-c723-4dec-e695-490cffcdbcc8"},"outputs":[],"source":["test.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":646,"status":"ok","timestamp":1711639847395,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"fyYON_FZiLIs","outputId":"a473e7e9-8a23-4000-8f32-f98294807011"},"outputs":[],"source":["test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":406,"status":"ok","timestamp":1711639851899,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"Oo8QV653iLIs"},"outputs":[],"source":["train[\"Review\"]= train[\"Review\"].astype(str)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":154208,"status":"ok","timestamp":1711640124598,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"Jcpmf08ZiLIs"},"outputs":[],"source":["train[\"Review\"] = train[\"Review\"].apply(preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":425,"status":"ok","timestamp":1711640130851,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"esXSL6P0iLIs","outputId":"3aa82ddd-f124-47e1-af7a-e0119344ca52"},"outputs":[],"source":["train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1711640134423,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"awR3j3bsiLIs","outputId":"cccf64e4-64f4-413b-af05-a9828ac515b9"},"outputs":[],"source":["y = train[\"overall\"]\n","X_train, X_test, y_train, y_test = train_test_split(train[\"Review\"], y, stratify=y,test_size=0.3, random_state=42)\n","print('Original class distribution:', Counter(y_train))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711640136039,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"VPyBJuAoiLIs","outputId":"18a068a9-1d5e-472b-e0de-0184216791cc"},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711640138423,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"LmpWSVKuiLIt"},"outputs":[],"source":["def create_pipeline(classifiers, vectorizer, normalizer=None, reduction= False, feature_selection=True,k=1000):\n","     # start building the pipeline, first step -> vectorization\n","    step =[(\"vectorizer\",vectorizer)]\n","     # if dimensionality reduction is requested add a TruncatedSVD step to the pipeline\n","    # if reduction:\n","    #     step.append((\"reduction\",TruncatedSVD()))\n","    # add normalization step\n","    if normalizer:\n","        step.append((\"normalizer\",normalizer))\n","    # if feature selection is requested add a SelectKBest step to the pipeline\n","    if feature_selection:\n","        step.append(('feature_selection', SelectKBest(chi2, k=k)))\n","    # add the classifier to the pipeline\n","    step.append((\"classifier\",classifiers))\n","\n","        # pipeline object with the steps prepared above and assign it to the corresponding classifier name in the models dict\n","    models = Pipeline(step)\n","    return models"]},{"cell_type":"markdown","metadata":{"id":"x5e9hqzsiLIt"},"source":["## Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"T96tXmlOiLIu"},"source":["### Comparing NGRAM Features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24080,"status":"ok","timestamp":1711640166079,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"5zIVRVEmiLIu","outputId":"7200efa7-0501-40bb-e893-476de293089f"},"outputs":[],"source":["class_weights = {1: 7, 2: 12, 3: 9, 4: 8, 5: 7}\n","classifier = LogisticRegression(max_iter=1000,penalty=\"l2\",C=0.13,solver= \"liblinear\" ,class_weight=class_weights)\n","model = create_pipeline(classifier, TfidfVectorizer(ngram_range=(1,1)),k=15000)\n","model.fit(X_train,y_train)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24882,"status":"ok","timestamp":1711640199851,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"Dg4s8GD8iLIx","outputId":"f848f227-d6e9-4ab2-8087-74b93f049936"},"outputs":[],"source":["class_weights = {1: 7, 2: 12, 3: 9, 4: 8, 5: 7}\n","classifier = LogisticRegression(max_iter=1000,penalty=\"l2\",C=0.13,solver= \"liblinear\" ,class_weight=class_weights)\n","model = create_pipeline(classifier, TfidfVectorizer(ngram_range=(2,2)),k=20000)\n","model.fit(X_train,y_train)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29928,"status":"ok","timestamp":1711640232041,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"dgQg1etaiLIx","outputId":"d4f0f6a9-2753-4025-a915-f1fbfc3c23c2"},"outputs":[],"source":["class_weights = {1: 7, 2: 12, 3: 9, 4: 8, 5: 7}\n","classifier = LogisticRegression(max_iter=1000,penalty=\"l2\",C=0.13,solver= \"liblinear\" ,class_weight=class_weights)\n","model = create_pipeline(classifier, TfidfVectorizer(ngram_range=(3,3)))\n","model.fit(X_train,y_train)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58372,"status":"ok","timestamp":1711640330945,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"xFn2Jv9xiLIx","outputId":"b3df71ff-2384-4e13-9939-23bd23e57d21"},"outputs":[],"source":["class_weights = {1: 7, 2: 12, 3: 9, 4: 8, 5: 7}\n","classifier = LogisticRegression(max_iter=1000,penalty=\"l2\",C=0.13,solver= \"liblinear\" ,class_weight=class_weights)\n","model = create_pipeline(classifier, TfidfVectorizer(ngram_range=(1,3)),k=24500)\n","model.fit(X_train,y_train)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))"]},{"cell_type":"markdown","metadata":{"id":"AGLtMM6RiLIy"},"source":["## XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1711640341579,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"jG_yRRt7iLIy"},"outputs":[],"source":["sampling_strategy = {i: 23000 for i in range(1, 4)}\n","undersampler = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n","X_train, y_train= undersampler.fit_resample(X_train, y_train)\n","\n","y_train_adjusted = y_train-1\n","y_test = y_test-1"]},{"cell_type":"markdown","metadata":{"id":"lDGxFIVLiLIy"},"source":["### NGram"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":666,"status":"ok","timestamp":1711640346291,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"SnYhBcr_iLIy"},"outputs":[],"source":["# space = {\n","#     'max_depth': hp.choice('max_depth', np.arange(3, 11, dtype=int)),\n","#     'n_estimators': hp.choice('n_estimators', np.arange(100, 1001, 100, dtype=int)),\n","#     'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n","#     'subsample': hp.uniform('subsample', 0.5, 1.0),\n","#     'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n","#     'min_child_weight': hp.choice('min_child_weight', np.arange(1, 11, dtype=int)),\n","#     'reg_lambda': hp.uniform('reg_lambda', 1, 100),\n","#     'k': hp.choice('k', np.arange(8000, 17000, 500, dtype=int))\n","# }\n","\n","# def objective(params):\n","#     k = params.pop('k')\n","#     feature_selector = SelectKBest(chi2, k=k)\n","#     X_train_sel = feature_selector.fit_transform(X_train, y_train)\n","#     X_test_sel = feature_selector.transform(X_test)\n","#     clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', **params)\n","#     score = cross_val_score(clf, X_train_sel, y_train_adjusted, scoring='accuracy', cv=5).mean()\n","#     print(f\"\\nTrial completed:\")\n","#     print(f\"Params: {params} features:{k}\")\n","#     print(f\"Accuracy: {score}, Loss: {-score}\")\n","#     return {'loss': -score, 'status': STATUS_OK}\n","# # Run the optimization\n","# trials = Trials()\n","# best = fmin(fn=objective,\n","#             space=space,\n","#             algo=tpe.suggest,\n","#             max_evals=50,\n","#             trials=trials)\n","\n","# print(\"Best hyperparameters:\", best)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":162800,"status":"error","timestamp":1711642020150,"user":{"displayName":"Sai Nikhita","userId":"01344125893615558680"},"user_tz":-240},"id":"VIlhdGlGiLIy","outputId":"6b70aa6c-3d3d-4bf6-d6b9-2461cf1665b8"},"outputs":[],"source":["params = {'colsample_bytree': 0.8230008586447808,\n"," 'learning_rate': 0.1909617073607323,\n"," 'max_depth': 10,\n"," 'min_child_weight': 4,\n"," 'n_estimators': 1000,\n"," 'reg_lambda': 4.163618957089685,\n"," 'subsample': 0.59102293915828 }\n","\n","classifier =  xgb.XGBClassifier(eval_metric='mlogloss', **params)\n","\n","model = create_pipeline(classifier, TfidfVectorizer(ngram_range=(1,3)),k=10500)\n","model.fit(X_train,y_train_adjusted)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))\n"]},{"cell_type":"markdown","metadata":{"id":"p9zpi0t4iLIy"},"source":["### Googles Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiZocrejiLIy"},"outputs":[],"source":["google_word2vec = KeyedVectors.load_word2vec_format(\"../GoogleNews-vectors-negative300.bin\", binary=True)\n","X_train = np.array([document_vecotr(google_word2vec,doc) for doc in X_train])\n","y_train = np.array([document_vecotr(google_word2vec,doc) for doc in y_train])\n","model.fit(X_train,y_train)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))\n"]},{"cell_type":"markdown","metadata":{"id":"O29q4IZ_iLIy"},"source":["## Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-Hlxka8iLIy"},"outputs":[],"source":["classifier =  RandomForestClassifier()\n","model = create_pipeline(classifier, CountVectorizer(),k=10500)\n","model.fit(X_train,y_train)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))"]},{"cell_type":"markdown","metadata":{"id":"8lTbyI3OwFWs"},"source":["## SGD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qe7sNlFwQjF"},"outputs":[],"source":["classifier =  SGDClassifier()\n","model = create_pipeline(classifier, CountVectorizer(),k=10500)\n","model.fit(X_train,y_train)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))"]},{"cell_type":"markdown","metadata":{"id":"mBRun7CBwKou"},"source":["## SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9H8VBTifwrCJ"},"outputs":[],"source":["classifier =  SGDClassifier()\n","model = create_pipeline(classifier, CountVectorizer(binary=True),k=10500)\n","model.fit(X_train,y_train)\n","y_pred = model.predict(X_test)\n","print(classification_report(y_pred,y_test))"]},{"cell_type":"markdown","metadata":{},"source":["# Sequence Model"]},{"cell_type":"markdown","metadata":{},"source":["## Bi-Directional LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## LSTM"]},{"cell_type":"markdown","metadata":{},"source":["## Transformers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import TrainingArguments, Trainer\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset\n","import torch\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Load pre-trained model\n","# # 5 classes\n","# num_labels = 5\n","# model_name = \"roberta-base\"\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","\n","# if torch.cuda.is_available():\n","#     print(f\"CUDA is available. Current device: {torch.cuda.current_device()} - {torch.cuda.get_device_name(0)}\")\n","#     device = torch.device(\"cuda\")\n","# else:\n","#     print(\"CUDA is not available, using CPU instead.\")\n","#     device = torch.device(\"cpu\")\n","\n","# # Move model to the chosen device\n","# model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df = pd.read_csv(\"train.csv\")\n","# df['Review'] = df['Review'].astype(str)\n","# df.dropna(subset=['Review', 'overall'], inplace=True)\n","# df['overall'] = df['overall'] - 1\n","\n","# # Split the dataset into training and testing\n","# train_df, test_df = train_test_split(df, test_size=0.3, stratify=df['overall'], random_state=42)\n","# train_dataset = Dataset.from_pandas(train_df)\n","# test_dataset = Dataset.from_pandas(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Define the tokenize function\n","# def tokenize_function(examples):\n","#     return {\n","#         **tokenizer(examples['Review'], truncation=True, padding=\"max_length\", max_length=512),\n","#         'labels': examples['overall']\n","#     }\n","\n","# # Tokenize the dataset\n","# tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n","# tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Define training arguments\n","# training_args = TrainingArguments(\n","#     output_dir='./results',\n","#     num_train_epochs=15,\n","#     per_device_train_batch_size=1,\n","#     gradient_accumulation_steps=8,\n","#     per_device_eval_batch_size=1,\n","#     warmup_steps=500,\n","#     weight_decay=0.01,\n","#     logging_dir='./logs',\n","#     logging_steps=10,\n","#     evaluation_strategy=\"steps\",\n","#     eval_steps=500,\n","#     save_strategy=\"steps\",\n","#     save_steps=500,\n","#     load_best_model_at_end=True,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Initialize and train the Trainer\n","# trainer = Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=tokenized_train_dataset,\n","#     eval_dataset=tokenized_test_dataset,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the model\n","# trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# trainer.evaluate()"]},{"cell_type":"markdown","metadata":{},"source":["## Model Testing"]},{"cell_type":"markdown","metadata":{},"source":["### Roberta-base with 3 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load the fine-tuned model and tokenizer\n","model_name = \"bert_base_init\"\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# loading test data\n","input_file = \"test.csv\"\n","df = pd.read_csv(input_file)\n","df['Review'] = df['Review'].astype(str)\n","\n","# Prepare the reviews for the model\n","def prepare_data(reviews):\n","    tokenized = tokenizer(reviews, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n","    return tokenized"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Classify reviews and add predictions to the DataFrame\n","def classify_reviews(df):\n","    predictions = []\n","    for review in df['Review']:\n","        inputs = prepare_data(review).to(device)\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            preds = torch.argmax(outputs.logits, dim=1)\n","            predictions.append(preds.item() + 1) \n","    df['overall'] = predictions\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply the classification\n","df = classify_reviews(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_file = \"prediction13.csv\"\n","df[['id', 'overall']].to_csv(output_file, index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Bert-base with 3 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_model =  \"bert_base_init\"\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels = 5)\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# loading test data\n","input_file = \"test.csv\"\n","df = pd.read_csv(input_file)\n","df['Review'] = df['Review'].astype(str)\n","\n","# Prepare the reviews for the model\n","def prepare_data(reviews):\n","    tokenized = tokenizer(reviews, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n","    return tokenized\n","\n","# Classify reviews and add predictions to the DataFrame\n","def classify_reviews(df):\n","    predictions = []\n","    for review in df['Review']:\n","        inputs = prepare_data(review).to(device)\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            preds = torch.argmax(outputs.logits, dim=1)\n","            predictions.append(preds.item() + 1)\n","    df['overall'] = predictions\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply the classification\n","df = classify_reviews(df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the DataFrame with predictions to a new CSV file\n","output_file = \"./predictions/prediction17.csv\"\n","df[['id', 'overall']].to_csv(output_file, index=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
